{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb420916-e459-4c48-bd93-19995adc4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from MNIST_loader import loadDataWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7bdbb9-7258-425f-bc59-d3b4b41d33df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(No seed given, using 78565 instead!)\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = loadDataWrapper()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2e5c37-c932-4198-8103-d2da86f4226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetMaker(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return X.shape(0)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index,:,:]\n",
    "        y = self.Y[index,:,:]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f99ec3-fe7d-4348-890a-370ebceb520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(epoch, optimizer, loss_function, model, train_loader):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data,label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        loss_value = loss_function(predictions,label)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_value.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(epoch, loss, model, val_loader):\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "            predictions = model(data)\n",
    "            loss_value = loss(predictions, label)\n",
    "            total_loss += loss_value.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def a_proper_training(num_epoch, model, optimizer, loss_function, train_loader, val_loader):\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "      \n",
    "        start_time = time.time()  # Start time\n",
    "        \n",
    "        val_loss = validate_epoch(epoch, loss_function, model, val_loader)\n",
    "        train_loss = train_epoch(epoch, optimizer, loss_function, model, train_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "  \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Epoch {epoch + 1}/{round(num_epoch, 10)}: Train Loss={round(train_loss,10)} Val Loss={round(val_loss,10)} Test Loss={round(test_loss,10)} Elapsed_time = {round(elapsed_time/60,2)}minutes\")\n",
    "    \n",
    "    return (best_model, best_epoch, train_losses, val_losses, test_losses, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7e90a2-dd1e-4dd3-a379-1ecbeaba0528",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the CNN model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(CNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # 1 input channel (grayscale), 16 output channels, 3x3 convolution\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # 16 input channels, 32 output channels, 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Fully connected layer that takes the flattened features and outputs a tensor of size 9\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 9)  # 32 channels * 7x7 after pooling\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution layer 1 + ReLU activation + 2x2 Max Pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Convolution layer 2 + ReLU activation + 2x2 Max Pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Flatten the feature maps into a 1D vector for the fully connected layer\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Flatten to batch size x 32*7*7\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7328fdc-0723-4971-adc0-c13a93679da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = datasetMaker(x_train, y_train)\n",
    "testset = datasetMaker(x_test, y_test)\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "# Now pass the scheduler to the training function\n",
    "best_model, best_epoch, train_losses, val_losses, test_losses, lrs = a_proper_training(\n",
    "    epochs, model, optimizer, criterion, train_loader, val_loader, test_loader, scheduler, verbose=verbose, patience=patience\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
